services:
  # Dolt SQL Server Database Service
  dolt-db:
    image: ${DOLT_IMAGE:-deploy-dolt-db}
    build:
      context: ..
      dockerfile: services/dolt_memory/cogni-cogni-dao-memory.dockerfile
    env_file: ./.env
    environment:
      - HOST=0.0.0.0
      - PORT=3306
      - DOLT_ROOT_PATH=/.dolt
      - DATABASE_REMOTE=https://doltremoteapi.dolthub.com/cogni/cogni-dao-memory
      - DATABASE_NAME=cogni-dao-memory
      - DOLTHUB_USER=cogni
      - DOLTHUB_EMAIL=steward@cognidao.org
      - DATA_DIR=/dolthub-dbs/cogni/cogni-dao-memory
      - DOLT_ROOT_HOST=%
      - DOLT_ROOT_PASSWORD=${DOLT_ROOT_PASSWORD}
      - CREDS_KEY=g3ld8if3kr1kl5ajm6ptl1kii5hvgo1t6vka5ms02arv2
      - CREDS_VALUE=${DOLTHUB_JWK_CREDENTIAL}
    volumes:
      - dolt_data:/dolthub-dbs
    ports:
      - "3306:3306"
    healthcheck:
      test: ["CMD", "dolt", "sql", "-q", "SELECT 1;"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    # Resource limits for Docker Compose (local development - enforced)
    mem_limit: 2G           # Dolt database memory limit
    cpus: 2.0               # 2 CPU cores max
    # Resource limits for Docker Swarm mode (production - future compatibility)
    deploy:
      resources:
        limits:
          memory: 2G      # Dolt database memory limit
          cpus: '2.0'     # 2 CPU cores max
        reservations:
          memory: 512M    # Minimum memory guarantee
          cpus: '0.5'     # Minimum CPU guarantee
    networks:
      - cogni-net

  # Redis Service for LangGraph Checkpointing
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    networks:
      - cogni-net

  # PostgreSQL Service for LangGraph Server Runtime
  postgres:
    image: postgres:16-alpine
    environment:
      - POSTGRES_DB=langgraph
      - POSTGRES_USER=langgraph
      - POSTGRES_PASSWORD=langgraph
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U langgraph"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    # Resource limits for Docker Compose (local development - enforced)
    mem_limit: 1G           # PostgreSQL memory limit
    cpus: 1.0               # 1 CPU core max
    # Resource limits for Docker Swarm mode (production - future compatibility)
    deploy:
      resources:
        limits:
          memory: 1G      # PostgreSQL memory limit
          cpus: '1.0'     # 1 CPU core max
        reservations:
          memory: 256M    # Minimum memory guarantee
          cpus: '0.25'    # Minimum CPU guarantee
    networks:
      - cogni-net

  # Web API Service  
  api:
    image: ${API_IMAGE:-cogni-api-local}
    build:
      context: ..
      dockerfile: services/web_api/Dockerfile.api
    env_file: ./.env
    environment:
      - DOLT_HOST=dolt-db
      - DOLT_PORT=3306
      - DOLT_USER=root
      - DOLT_PASSWORD=${DOLT_ROOT_PASSWORD}
      - DOLT_DATABASE=cogni-dao-memory
      - DOLT_REMOTE_URL=https://doltremoteapi.dolthub.com/cogni/cogni-dao-memory
      - DOLT_REMOTE_PASSWORD=${DOLTHUB_MCP_ACCESS_WRITE}
      # ToolHive API URL for MCP tool health checking
      - TOOLHIVE_API=http://toolhive:8080
    ports: ["8000:8000"]  # Direct access for deploy script compatibility
    depends_on:
      dolt-db:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/healthz')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - cogni-net

  caddy:
    image: caddy:2-alpine
    ports: ["80:80", "443:443"]
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    depends_on: [api]
    networks:
      - cogni-net

  # Prefect Server with SQLite
  prefect-server:
    image: prefecthq/prefect:3-python3.12
    command: prefect server start --host 0.0.0.0
    environment:
      - PREFECT_UI_URL=http://localhost:4200
      - PREFECT_API_URL=http://localhost:4200/api
      - PREFECT_SERVER_API_HOST=0.0.0.0
      - PREFECT_SERVER_API_PORT=4200
    ports:
      - "4200:4200"  # Direct access like you're used to
    volumes:
      - prefect_data:/root/.prefect  # Persist SQLite database
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4200/api/health')"]
      interval: 30s
      retries: 3
    networks:
      - cogni-net

  # Prefect Worker - connects to containerized Prefect server
  prefect-worker:
    image: ${PREFECT_WORKER_IMAGE:-cogni-prefect-worker-local}
    build:
      context: ..
      dockerfile: services/prefect_worker/Dockerfile.prefect-worker
    command: prefect worker start --pool cogni-pool
    env_file: ./.env
    environment:
      - PREFECT_API_URL=http://prefect-server:4200/api
      # ToolHive API URL for MCP tool access
      - TOOLHIVE_API=http://toolhive:8080
      # OpenAI API Key for AutoGen flows (explicit override)
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # Helicone observability configuration (sitecustomize.py will auto-configure OpenAI SDK)
      - HELICONE_API_KEY=${HELICONE_API_KEY}
      - HELICONE_BASE_URL=${HELICONE_BASE_URL:-https://oai.helicone.ai/v1}
    depends_on:
      prefect-server:
        condition: service_healthy
    volumes:
      - ../:/workspace  # Mount the entire project for flow access
      - ./.env:/workspace/.env  # Explicitly mount .env file
    working_dir: /workspace
    restart: unless-stopped
    networks:
      - cogni-net

  # ToolHive - MCP container orchestrator  
  toolhive:
    image: ghcr.io/stacklok/toolhive:latest
    container_name: toolhive
    command: ["serve", "--host", "0.0.0.0", "--port", "8080"]  # Bind to all interfaces
    user: "0:0"  # Run as root for Docker socket access on macOS
    networks:
      - cogni-net
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:rw
    environment:
      - DOCKER_NETWORK=deploy_cogni-net
      - THV_SECRETS_BACKEND=1password
      - THV_NO_UPDATES=1  # Prevent version checks to avoid startup lag
    ports:
      - "8080:8080"  # Use ToolHive's default port 8080
      # MCP server ports are dynamically managed by ToolHive when servers are deployed
    restart: unless-stopped

  # LangGraph Combined Service (cogni_presence + playwright_poc)
  langgraph-cogni-presence:
    image: ${LANGGRAPH_COMBINED_IMAGE:-cogni-langgraph-combined-local}
    env_file: ./.env
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LANGSMITH_API_KEY=${LANGSMITH_API_KEY}
      - REDIS_URI=redis://redis:6379
      - DATABASE_URI=postgresql://langgraph:langgraph@postgres:5432/langgraph
      - COGNI_MCP_URL=http://toolhive:24160/sse
      - PLAYWRIGHT_MCP_URL=http://toolhive:24162/sse
      - PREFECT_API_URL=http://prefect-server:4200/api
      - LOG_LEVEL=debug
      # MCP Client Reconnection Configuration
      - MCP_MAX_RETRIES=5
      - MCP_HEALTH_CHECK_INTERVAL=30.0
      - MCP_CONNECTION_TIMEOUT=30.0
    ports:
      - "8002:8000"  # Both graphs accessible via same service (Web API connects here)
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      toolhive:
        condition: service_started
    restart: unless-stopped
    networks:
      - cogni-net

  # Qwen3 Local Model Service via Ollama (Production-ready)
  qwen-ollama:
    image: ollama/ollama:latest
    container_name: qwen-ollama
    ports:
      - "7869:11434"  # Non-conflicting port mapping
    volumes:
      - ollama_models:/root/.ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    environment:
      - OLLAMA_KEEP_ALIVE=24h  # Prevent model unloading
      - OLLAMA_HOST=0.0.0.0    # Bind to all interfaces
    # Resource limits for Docker Compose (local development - enforced)
    mem_limit: 6G           # Model + inference memory limit
    cpus: 4.0               # 4 CPU cores max
    # Resource limits for Docker Swarm mode (production - future compatibility)
    deploy:
      resources:
        limits:
          memory: 6G      # Generous for 7B model + overhead
          cpus: '4.0'     # 4 CPU cores max
        reservations:
          memory: 2G      # Minimum memory guarantee
          cpus: '1.0'     # Minimum CPU guarantee
    networks:
      - cogni-net
    # Uncomment for GPU support (requires Docker Compose 2.23+)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # Model initialization service (runs once to pull models)
  qwen-init:
    image: ollama/ollama:latest
    container_name: qwen-init
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=qwen-ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen3:4b}
    depends_on:
      - qwen-ollama
    command: >
      sh -c '
        echo "⏳ Waiting for Ollama server to be ready..." &&
        until OLLAMA_HOST=qwen-ollama:11434 ollama list >/dev/null 2>&1; do
          echo "   Waiting for Ollama server..."
          sleep 5
        done &&
        echo "📦 Pulling model: $${OLLAMA_MODEL}..." &&
        OLLAMA_HOST=qwen-ollama:11434 ollama pull $${OLLAMA_MODEL} &&
        echo "✅ Model ready: $${OLLAMA_MODEL}"
      '
    restart: "no"  # Run once only
    networks:
      - cogni-net 

volumes:
  caddy_data:
  caddy_config:
  dolt_data:
  prefect_data:
  mcp_chroma:
  redis_data:
  ollama_models:

networks:
  cogni-net:
    driver: bridge 