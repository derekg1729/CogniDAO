"""
Test for the INVALID_PROMPT_INPUT bug with ChatPromptTemplate.

This test reproduces the KeyError that occurs when LangGraph tries to use
a ChatPromptTemplate that expects 'task_context' but receives LangGraph's
standard variables: ['messages', 'is_last_step', 'remaining_steps'].
"""

import pytest
from langchain_core.messages import HumanMessage

from src.cogni_presence.graph import build_compiled_graph


class TestPromptTemplateBug:
    """Test cases for the ChatPromptTemplate variable mismatch bug."""

    @pytest.mark.asyncio
    async def test_agent_creation_with_messages_works(self):
        """
        Test that the fixed prompt template works with LangGraph's expected input variables.
        
        This test should PASS now that the prompt template is fixed.
        """
        # Just test that the agent can be created without errors
        # The template no longer has task_context issues
        agent_node = await create_agent_node()
        assert agent_node is not None
        
        # Test that the agent has the correct structure
        assert hasattr(agent_node, 'ainvoke')
        
        # Verify the prompt template has the correct variables
        from src.cogni_presence.prompts import COGNI_PRESENCE_PROMPT
        expected_vars = COGNI_PRESENCE_PROMPT.input_variables
        assert 'messages' in expected_vars
        assert 'tool_specs' in expected_vars
        assert 'task_context' not in expected_vars

    @pytest.mark.asyncio 
    async def test_prompt_template_works_with_messages(self):
        """
        Test that verifies our fixed prompt template works with messages.
        
        This demonstrates that after the fix, the template works with LangGraph's variables.
        """
        from src.cogni_presence.prompts import COGNI_PRESENCE_PROMPT
        from src.shared_utils.tool_specs import generate_tool_specs_from_mcp_tools
        
        # Create a partial prompt like our agent does
        tool_specs = generate_tool_specs_from_mcp_tools([])
        partial_prompt = COGNI_PRESENCE_PROMPT.partial(tool_specs=tool_specs)
        
        # Try to format with LangGraph's variables (should work now)
        langgraph_variables = {
            'messages': [HumanMessage(content="Hello")],
        }
        
        # This should work without raising an error
        result = partial_prompt.format(**langgraph_variables)
        assert result is not None
        assert "Hello" in result

    def test_prompt_template_structure_analysis(self):
        """
        Analyze what variables our fixed prompt template actually expects.
        
        This verifies that the fix is working correctly.
        """
        from src.cogni_presence.prompts import COGNI_PRESENCE_PROMPT
        
        # Get the input variables that the template expects
        expected_vars = COGNI_PRESENCE_PROMPT.input_variables
        print(f"Prompt expects these variables: {expected_vars}")
        
        # This should include 'messages' and 'tool_specs' (not task_context)
        assert 'messages' in expected_vars
        assert 'tool_specs' in expected_vars
        assert 'task_context' not in expected_vars