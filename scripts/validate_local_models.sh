#!/bin/bash
# Validation script for Local Model Integration (Production-ready with init container)
# Tests external API access and model availability for agent-based flows

set -e

echo "ğŸ” Validating Local Model Setup..."

# Check if Ollama container is running
echo "ğŸ“¦ Checking container status..."
if ! docker-compose -f deploy/docker-compose.yml ps qwen-ollama | grep -q "Up"; then
    echo "âŒ qwen-ollama container not running"
    exit 1
fi

echo "âœ… Ollama container is running"

# Check if init container completed successfully
echo "ğŸ“¦ Checking model initialization status..."
INIT_STATUS=$(docker-compose -f deploy/docker-compose.yml ps qwen-init --format "table {{.State}}" | tail -n +2 | tr -d ' ')
if [[ "$INIT_STATUS" == "Exited(0)" ]]; then
    echo "âœ… Model initialization completed successfully"
elif [[ "$INIT_STATUS" == "Exited"* ]]; then
    echo "âŒ Model initialization failed - check logs:"
    echo "ğŸ’¡ Try: docker-compose -f deploy/docker-compose.yml logs qwen-init"
    exit 1
else
    echo "â³ Model initialization still in progress..."
    echo "ğŸ’¡ Check progress: docker-compose -f deploy/docker-compose.yml logs qwen-init -f"
fi

# Test external API access
echo "ğŸŒ Testing external API access (localhost:7869)..."
if curl -f http://localhost:7869/api/tags >/dev/null 2>&1; then
    echo "âœ… External API accessible"
else
    echo "âŒ External API not accessible"
    echo "ğŸ’¡ Try: docker-compose -f deploy/docker-compose.yml logs qwen-ollama"
    exit 1
fi

# Test model availability
echo "ğŸ¤– Testing model availability..."
MODEL_NAME="${OLLAMA_MODEL:-qwen3:4b}"
if curl -s http://localhost:7869/api/tags | grep -q "$MODEL_NAME"; then
    echo "âœ… Model '$MODEL_NAME' is available"
else
    echo "âŒ Model '$MODEL_NAME' not found"
    echo "ğŸ’¡ Available models:"
    curl -s http://localhost:7869/api/tags | jq -r '.models[].name' 2>/dev/null || echo "   Could not list models"
    exit 1
fi

# Test OpenAI-compatible API
echo "ğŸ”Œ Testing OpenAI-compatible API..."
RESPONSE=$(curl -s -X POST http://localhost:7869/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d "{\"model\": \"$MODEL_NAME\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}], \"max_tokens\": 10}" \
    || echo "ERROR")

if [[ "$RESPONSE" != "ERROR" ]] && echo "$RESPONSE" | jq -e '.choices[0].message.content' >/dev/null 2>&1; then
    echo "âœ… OpenAI-compatible API working"
else
    echo "âŒ OpenAI-compatible API failed"
    echo "Response: $RESPONSE"
    exit 1
fi

echo ""
echo "ğŸ‰ Local Model Integration Validation Complete!"
echo ""
echo "ğŸ“‹ Summary:"
echo "   ğŸŸ¢ Ollama API: http://localhost:7869"
echo "   ğŸŸ¢ Model: $MODEL_NAME"
echo "   ğŸŸ¢ OpenAI API: http://localhost:7869/v1/chat/completions"
echo ""
echo "ğŸ”— LangGraph Configuration:"
echo "   LLM_PROVIDER=ollama"
echo "   OLLAMA_URL=http://qwen-ollama:11434  # Internal Docker network"
echo "   OLLAMA_MODEL=$MODEL_NAME" 