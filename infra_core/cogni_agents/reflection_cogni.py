"""
ReflectionCogniAgent implementation for Ritual of Presence

This agent responds to the thoughts generated by CoreCogniAgent,
using shared memory.
"""

from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

# Import base agent and memory adapter
from infra_core.cogni_agents.base import CogniAgent
from infra_core.memory.memory_bank import CogniLangchainMemoryAdapter # Needed for type hint/checks

# Import OpenAI handler specifics
from infra_core.openai_handler import initialize_openai_client, create_completion, extract_content


class ReflectionCogniAgent(CogniAgent):
    """
    ReflectionCogniAgent responds to thoughts from CoreCogniAgent.
    
    It accesses shared memory to retrieve the previous thought and generate a reflection.
    """
    
    def __init__(self, agent_root: Path, memory_adapter: CogniLangchainMemoryAdapter, memory_bank_root_override: Optional[Path] = None, project_root_override: Optional[Path] = None):
        """
        Initialize a new ReflectionCogniAgent.
        
        Args:
            agent_root: Root directory for agent outputs (reflection files, though likely unused directly).
            memory_adapter: The shared CogniLangchainMemoryAdapter instance.
            memory_bank_root_override: Optional override for memory bank root path (passed to base, but adapter is primary).
            project_root_override: Optional override for project root path.
        """
        super().__init__(
            name="reflection-cogni",
            # This agent might not need its own specific spirit, 
            # but core context might still be useful.
            spirit_path=Path("infra_core/cogni_spirit/spirits/cogni-core-spirit.md"), 
            agent_root=agent_root,
            memory_bank_root_override=memory_bank_root_override,
            project_root_override=project_root_override
        )
        # Crucially, store the passed-in adapter. Base class uses CogniMemoryBank directly.
        # We need the adapter for LangChain message handling.
        self.memory_adapter = memory_adapter 
        self.openai_client = None
    
    def _initialize_client(self):
        """Initialize OpenAI client if not already initialized."""
        if self.openai_client is None:
            self.openai_client = initialize_openai_client()
        
        # Load core context if needed (optional, depending on desired behavior)
        # if self.core_context is None:
        #     self.load_core_context()

    def prepare_input(self, *args, **kwargs) -> Dict[str, Any]:
        """
        Prepare inputs for reflection generation.
        Retrieves the previous thought from memory.
        
        Returns:
            Dictionary with prepared inputs, including the previous thought.
        """
        self._initialize_client()
        
        # Load history using the adapter
        memory_vars = self.memory_adapter.load_memory_variables({}) # input arg usually empty
        history_messages = memory_vars.get("history", [])
        
        last_thought = "" 
        if history_messages:
            # Assuming the last message is the thought from CoreCogniAgent
            last_message = history_messages[-1]
            last_thought = last_message.content
        else:
            print("Warning: No previous thought found in memory for reflection.")
            last_thought = "I sense silence. There was no preceding thought to reflect upon."

        prompt = f"Reflect on the following thought: \n\n> {last_thought}\n\nKeep your reflection concise, under 280 characters."
        
        return {
            "prompt": prompt,
            "previous_thought": last_thought, # Pass for potential use in act
            "temperature": 0.7 # Slightly different temperature for variety
        }
    
    def act(self, prepared_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate a reflection based on the previous thought.
        
        Args:
            prepared_input: Dictionary with prompt and previous thought
            
        Returns:
            Dictionary with the reflection content and metadata
        """
        prompt = prepared_input.get("prompt")
        temperature = prepared_input.get("temperature", 0.7)
        previous_thought = prepared_input.get("previous_thought", "[Previous thought not available]")
        
        # Ensure client is loaded 
        self._initialize_client() 
        
        reflection_content = "Default reflection if generation fails."
        try:
            # Call OpenAI API - System prompt might be useful here too
            # Using core_context if loaded, otherwise a default system message
            system_message = self.core_context['context'] if self.core_context else {"role": "system", "content": "You are a reflective AI assistant."}
            
            response = create_completion(
                client=self.openai_client,
                system_message=system_message,
                user_prompt=prompt, # The prompt constructed in prepare_input already contains the previous thought
                temperature=temperature
            )
            
            reflection_content = extract_content(response)
            
        except Exception as e:
            reflection_content = f"Error encountered during reflection: {str(e)}"
            print(f"Error during OpenAI call: {e}") 
        
        timestamp = datetime.utcnow()
        timestamp_str = timestamp.strftime("%Y-%m-%d-%H-%M")
        
        result_data = {
            "timestamp": timestamp_str,
            "reflection_content": reflection_content,
            "reflected_on_thought": previous_thought,
            "prompt_used": prompt, 
            "temperature_used": temperature
        }
        
        # Use record_action from base class to save output and log to the CORE memory bank
        # The adapter handles loading, but saving still uses the base agent's 
        # connection to the core bank.
        # Note: The adapter's save_context isn't called directly here because
        # this agent's action isn't a direct response to a user *input* in the LangChain sense.
        # We are logging the agent's own generated output.
        output_path = self.record_action(result_data, prefix="reflection_")
        
        result_data["filepath_logged_to_memory"] = str(output_path) # Record the intended path
        return result_data

    # Override format_output_markdown if a different structure is desired
    # Otherwise, it will use the base class's format.
    # def format_output_markdown(self, data: Dict[str, Any]) -> str:
    #     ... 